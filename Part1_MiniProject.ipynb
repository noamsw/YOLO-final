{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\mat}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\mattr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\matinv}[1]{\\boldsymbol {#1}^{-1}}\n",
    "\\newcommand{\\vec}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\vectr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\rvar}[1]{\\mathrm {#1}}\n",
    "\\newcommand{\\rvec}[1]{\\boldsymbol{\\mathrm{#1}}}\n",
    "\\newcommand{\\diag}{\\mathop{\\mathrm {diag}}}\n",
    "\\newcommand{\\set}[1]{\\mathbb {#1}}\n",
    "\\newcommand{\\cset}[1]{\\mathcal{#1}}\n",
    "\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "\\newcommand{\\pderiv}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\bb}[1]{\\boldsymbol{#1}}\n",
    "\\newcommand{\\E}[2][]{\\mathbb{E}_{#1}\\left[#2\\right]}\n",
    "\\newcommand{\\ip}[3]{\\left<#1,#2\\right>_{#3}}\n",
    "\\newcommand{\\given}[]{\\,\\middle\\vert\\,}\n",
    "\\newcommand{\\DKL}[2]{\\cset{D}_{\\text{KL}}\\left(#1\\,\\Vert\\, #2\\right)}\n",
    "\\newcommand{\\grad}[]{\\nabla}\n",
    "$$\n",
    "\n",
    "# Part 1: Mini-Project\n",
    "<a id=part3></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part you'll implement a small comparative-analysis project, heavily based on the materials from the tutorials and homework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guidelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You should implement the code which displays your results in this notebook, and add any additional code files for your implementation in the `project/` directory. You can import these files here, as we do for the homeworks.\n",
    "- Running this notebook should not perform any training - load your results from some output files and display them here. The notebook must be runnable from start to end without errors.\n",
    "- You must include a detailed write-up (in the notebook) of what you implemented and how. \n",
    "- Explain the structure of your code and how to run it to reproduce your results.\n",
    "- Explicitly state any external code you used, including built-in pytorch models and code from the course tutorials/homework.\n",
    "- Analyze your numerical results, explaining **why** you got these results (not just specifying the results).\n",
    "- Where relevant, place all results in a table or display them using a graph.\n",
    "- Before submitting, make sure all files which are required to run this notebook are included in the generated submission zip.\n",
    "- Try to keep the submission file size under 10MB. Do not include model checkpoint files, dataset files, or any other non-essentials files. Instead include your results as images/text files/pickles/etc, and load them for display in this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object detection on TACO dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TACO is a growing image dataset of waste in the wild. It contains images of litter taken under diverse environments: woods, roads and beaches.\n",
    "\n",
    "<center><img src=\"imgs/taco.png\" /></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can read more about the dataset here: https://github.com/pedropro/TACO\n",
    "\n",
    "and can explore the data distribution and how to load it from here: https://github.com/pedropro/TACO/blob/master/demo.ipynb\n",
    "\n",
    "\n",
    "The stable version of the dataset that contain 1500 images and 4787 annotations exist in `datasets/TACO-master`\n",
    "You do not need to download the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project goals:\n",
    "\n",
    "* You need to perform Object Detection task, over 7 of the dataset.\n",
    "* The annotation for object detection can be downloaded from here: https://github.com/wimlds-trojmiasto/detect-waste/tree/main/annotations.\n",
    "* The data and annotation format is like the COCOAPI: https://github.com/cocodataset/cocoapi (you can find a notebook of how to perform evalutation using it here: https://github.com/cocodataset/cocoapi/blob/master/PythonAPI/pycocoEvalDemo.ipynb)\n",
    "(you need to install it..)\n",
    "* if you need a beginner guild for OD in COCOAPI, you can read and watch this link: https://www.neuralception.com/cocodatasetapi/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do i need to do?\n",
    "\n",
    "* **Everything is in the game!** as long as your model does not require more then 8 GB of memory and you follow the Guidelines above.\n",
    "\n",
    "\n",
    "### What does it mean?\n",
    "* you can use data augmentation, rather take what's implemented in the directory or use external libraries such as https://albumentations.ai/ (notice that when you create your own augmentations you need to change the annotation as well)\n",
    "* you can use more data if you find it useful (for examples, reviwew https://github.com/AgaMiko/waste-datasets-review)\n",
    "\n",
    "\n",
    "### What model can i use?\n",
    "* Whatever you want!\n",
    "you can review good models for the coco-OD task as a referance:\n",
    "SOTA: https://paperswithcode.com/sota/object-detection-on-coco\n",
    "Real-Time: https://paperswithcode.com/sota/real-time-object-detection-on-coco\n",
    "Or you can use older models like YOLO-V3 or Faster-RCNN\n",
    "* As long as you have a reason (complexity, speed, preformence), you are golden.\n",
    "\n",
    "### Tips for a good grade:\n",
    "* start as simple as possible. dealing with APIs are not the easiest for the first time and i predict that this would be your main issue. only when you have a running model that learn, you can add learning tricks.\n",
    "* use the visualization of a notebook, as we did over the course, check that your input actually fitting the model, the output is the desired size and so on.\n",
    "* It is recommanded to change the images to a fixed size, like shown in here :https://github.com/pedropro/TACO/blob/master/detector/inspect_data.ipynb\n",
    "* Please adress the architecture and your loss function/s in this notebook. if you decided to add some loss component like the Focal loss for instance, try to show the results before and after using it.\n",
    "* Plot your losses in this notebook, any evaluation metric can be shown as a function of time and possibe to analize per class.\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: This is where you should write your explanations and implement the code to display the results.\n",
    "See guidelines about what to include in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLOv8\n",
    "The model we chose for our project is the updated version of yolo that we saw in class. YOLOv8 is a state-of-the-art object detection and image segmentation model created by Ultralytics, the developers of YOLOv5, and launched on January 10, 2023.\n",
    "This new version features many improvements, such as:\n",
    "- A new backbone network based on ResNet-101 with attention modules\n",
    "- A design that makes it easy to compare model performance with older models in the YOLO family\n",
    "- A new loss function that combines cross-entropy, IoU and Dice losses\n",
    "- A new data augmentation technique called MixUp that blends images and labels from different classes\n",
    "\n",
    "YOLOv8 is designed to be fast, accurate, and easy to use, making it an excellent choice for our project. It is especially suited for image classification tasks. YOLOv8 is a modern and powerful model that can handle the complexity and diversity of the TACO dataset. We also wanted to explore the new features and improvements that YOLOv8 offers over previous versions. In this notebook, we will show you how we trained and evaluated YOLOv8 on the TACO dataset, and compare our results with other models. We will also discuss the challenges and limitations we faced, and suggest some possible directions for future work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "<figure>\n",
    "    <center><img src=\"imgs/architecturepic.jpg\" /></center>\n",
    "    <figcaption>\n",
    "    Courtesy of <a href=\"https://github.com/ultralytics/ultralytics/issues/189\">RangeKing</a>\n",
    "    </figcaption>\n",
    "</figure>\n",
    "YOLOv8 made some changes to the overall architecture of the previous models. \n",
    "In general the backbone of YOLO models is a modified version of the CSPDarknet53 architecture, which is a convolutional neural network that uses cross-stage partial connections to reduce redundancy and increase efficiency. The backbone actually consists of 5 layers, each with different architectures. The results are then sent to the head for prediction.\n",
    "\n",
    "### Activation function\n",
    "YOLO models also use the Mish activation function,  \n",
    "defined as: $f(x) = x tanh(softplus(x))$\n",
    "The Mish activation was used as it is smoother and easier to compute. \n",
    "\n",
    "### Head\n",
    "The head of YOLOv8 is composed of three detection branches, each with a different output resolution and scale. The detection branches use SPP (Spatial Pyramid Pooling) modules, which aggregate features from different levels of the feature pyramid to enhance the receptive field and robustness. The detection branches also use PANet (Path Aggregation Network) modules, which fuse features from the backbone and the previous branches to improve the feature quality. The head is now anchor free, leading to better accuracy.\n",
    "\n",
    "### Mosiac augmentation\n",
    "Mosaic augmentation, in which 4 images are stiched together was an important part of YOLOs initaial success. However it is now understood that turning it off for the last few training epochs produces better results.\n",
    "\n",
    "### Module and Convolution changes\n",
    "- Replace the C3 module with the C2f module\n",
    "- Replace the first 6x6 Conv with 3x3 Conv in the Backbone\n",
    "- Replace the first 1x1 Conv with 3x3 Conv in the Bottleneck\n",
    "- Use decoupled head and delete the objectness branch\n",
    "\n",
    "\n",
    "### Fun features\n",
    "YOLOv8 introduces a new feature called FPNAS (Feature Pyramid Network with AutoShape), which automatically adjusts the input image size and the anchor box shapes according to the dataset statistics. This allows YOLOv8 to adapt to different datasets and scenarios without manual tuning, and makes it very versatile.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss functions\n",
    "YOLOv8 has a number of different loss functions.\n",
    "\n",
    "##### Box loss\n",
    "This is the mean squared error (MSE) between the predicted box coordinates ( $x,y,w,h$ ) and the ground truth box coordinates ( $x∗,y∗,w∗,h∗$ ). It measures how well the model can localize the objects in the image. \n",
    "It is computed as:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{box}} = \\frac{1}{N} \\sum_{i=1}^{N} \\left[ (x_i - x_i^*)^2 + (y_i - y_i^*)^2 + (w_i - w_i^*)^2 + (h_i - h_i^*)^2 \\right]\n",
    "$$\n",
    "where $N$ is the number of boxes in the batch\n",
    "\n",
    "\n",
    "##### Varifocal  loss\n",
    "This is the loss between the predicted and ground truth class probabilities. YOLOv8 uses Varifocal Loss (VFL) as the classification loss. VFL is a focal loss variant that adapts to the quality-aware predictions.It attempts to predict the Intersection-over-Area-of-Union Score (IACS). It dynamically adjusts the focal parameter according to the prediction quality, which is measured by the predicted confidence score. VFL can reduce the impact of easy negatives and hard positives, and focus more on hard negatives and easy positives. VFL can also handle class imbalance and noisy labels better than focal loss.\n",
    "It is computed as:\n",
    "$$\n",
    "VFL(p,q) = \\begin{cases}\n",
    "-q(q\\log p + (1-q)\\log(1-p)) & \\text{if } q > 0 \\\\\n",
    "-\\alpha(1-p)^\\gamma\\log p & \\text{if } q = 0\n",
    "\\end{cases}\n",
    "$$\n",
    "where $p$ is the predicted $IACS$, $q$ is the ground truth $IACS$, $α$ and $γ$ are hyperparameters.\n",
    "\n",
    "The loss was based on this [paper](https://arxiv.org/abs/2008.13367)\n",
    "\n",
    "\n",
    "##### CIoU loss\n",
    "Complete IoU Loss is a bounding box regression loss that considers overlap, aspect ratio, and distance between the predicted and ground truth boxes. CIOU can handle various bounding box shapes and sizes, and penalize inaccurate predictions more effectively.\n",
    "It is computed as:\n",
    "$$\n",
    "\\mathcal{L}_{\\text{CIoU}} = 1 - \\text{IoU}(B, B^*) + \\frac{\\rho(B_c, B_c^*)}{c^2} + \\alpha v\n",
    "$$\n",
    "where $IoU(B,B^∗)$ is the intersection over union between $B$ and $B^∗$, $ρ(Bc​,B_c^∗​)$ is the Euclidean distance between the center points of $B$ and $B^∗$, $c$ is the diagonal length of the smallest enclosing box that covers both $B$ and $B^∗$, $α$ is a trade-off parameter, and $v$ is a penalty term for aspect ratio consistency.\n",
    "\n",
    "##### DFL loss\n",
    "This is the distribution focal loss (DFL) between the predicted class probabilities $p$ and the ground truth class probabilities $p^∗$. It is a distribution-based loss that models the label distribution as a Dirichlet distribution. It aims to align the predicted class probabilities with the target class probabilities. It penalizes misalignment between $p_j$​ and $q_j$​, where $q_j​=p_j^∗​+ϵ$, and $ϵ$ is a small positive constant.\n",
    "It is computed as:\n",
    "$$\n",
    "\\mathcal{L}_{\\text{DFL}} = - \\sum_{i=1}^{C} \\alpha_i (p_i - q_i)^{\\gamma} \\log(p_i)\n",
    "$$\n",
    "where $C$ is the number of classes, $α_i​$ is a scaling factor, and $γ$ is a focusing parameter.\n",
    "\n",
    "The loss was based on this [paper](https://ieeexplore.ieee.org/document/9792391)\n",
    "\n",
    "#### Total loss\n",
    "The total loss is computed by summing up the weighted classification loss, localization loss, and confidence loss for all bounding boxes across all images in a batch. \n",
    "It is computed as:\n",
    "$$\n",
    "\\mathcal{L}_{\\text{total}} = \\lambda_{\\text{box}} \\mathcal{L}_{\\text{box}} + \\lambda_{\\text{cls}} \\mathcal{L}_{\\text{cls}} + \\lambda_{\\text{reg}} \\mathcal{L}_{\\text{reg}}\n",
    "$$\n",
    "where $λ_{box}​,λ_{cls​},λ_{reg​}$ are the weights for each loss term, and $L_{reg​}=L_{CIoU​}+L_{DFL​}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "In order to train our model, we first had to do some preproccesing to the TACO dataset. The raw TACO data set comes as multiple batches of images with overlapping names. YOLO requires one folder of images for training, and one for testing. First we consolidated the images into one large folder, updating the image names in the annotation files as well. \n",
    "\n",
    "\n",
    "All of this was done by running the preprocessing script we wrote on the TACO dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# process the TACO dataset, unifying the image folders and updating the annotation files for use in roboflow\n",
    "# in order to run this script, simply provide the path to the folder containing both the TACO images\n",
    "# and annotations. Two new folders will be created, train_images and test_images, \n",
    "# annotation files will be updated inplace\n",
    "# replace DATAFOLDERPATH with the path to your data.\n",
    "# the DATAPATH must contain the batches of images as downloaded from the TACO repo, and the train and test annotation files we were referred to\n",
    "# %run project.preprocess.py DATAFOLDERPATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmentations and Train, Val and Test folders\n",
    "In order to augment the data set, and split the data into the relevant folders, we used Roboflow.\n",
    "Roboflow is the API recommended by Ultralytics. This Tool provides an easy online way to augment, resize, and split custom datasets. We uploaded the processed TACO dataset and annotations to Roboflow, where we used their online tools to do the following:\n",
    "- Resize all images to 640 $\\times$ 640, the recommeded size for YOLOv8\n",
    "- Auto Orient: Roboflow will automatically orient the images based on the information provided in annotations\n",
    "- Split the data into 90% training data and 10% validation dat\n",
    "\n",
    "Once the augmentations were applied, the dataset was ready for the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dowloading Dataset\n",
    "After using roboflow to augment and resize the data, the new data set can be downloaded using roboflows API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#Use this block to download the dataset\n",
    "#running this block will download the image and label files in the folder you are in\n",
    "#import project.model_utils as utils\n",
    "#dataset = utils.download_data(apikey=\"tbgnNS8bCW5iRz1lVg3O\", workspace=\"deep-learning-q1acw\",\n",
    "#                   project=\"trash-detection-original-train-images\", version=1, model=\"yolov8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model on the original dataset\n",
    "Once we have the dataset, we can begin training. This is straight forward to do using the YOLOv8 API\n",
    "Yolo will create a configuration file .config/ultralytics/settings.yaml that contains a path to the folder where the first download took place.\n",
    "It is important that all downloads must are done to there in order for the notebook to run smoothly and automatically. \n",
    "this is the full path to the folder **containing** the folders with the data, not the folder of data itself.\n",
    "Roboflow should create a download folder with another configuration file called data.yaml, this is the path you give to the model.\n",
    "If all went well in the download process it should work smoothly. \n",
    "\n",
    "otherwise you will get an error explaining what needs to be done. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#Here we initialize and train a model for 150 epochs, this will take a long time if run\n",
    "# it will also automatically validate the model on the validation set. \n",
    "#model = utils.initialize_and_train_model(dataset, 'yolov8x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "Here are the results of the models performance on the training set per Epoch as graphs:\n",
    "<figure>\n",
    "    <center><img src=\"imgs/resultsOriginalTrain1.png\" /></center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations\n",
    "The model behaves as expected, with the various loss functions decreases as we progress through the epochs, and evaluation metrics increase.\n",
    "The model was given an 150 epochs limit, but stopped training after 75, after not observing an improvement for 25 epochs.\n",
    "The validation subset is used for hyperparameter tuning and does not affect the weights directly. \n",
    "During our early test we noticed that CLS loss tended to overfit, and therefor adjusted the weight associated to it accordingly. \n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "The confusion matrix of best results we got:\n",
    "\n",
    "<figure>\n",
    "    <center><img src=\"imgs/confMatOrig.png\" /></center>\n",
    "</figure>\n",
    "\n",
    "As we can see some of the classes where not represented in the training data, and therefor have no rows.\n",
    "Some of the data is overrepresented, and results in many false postives. These are background, a category that is added automatically. \n",
    "\n",
    "We could lower the false positive rate by learning for longer, but we then run the risk of overfitting. The split of the categories is also simply not ideal. Some of the categories are severly under represented, and some over represented. The model simply does not have enough trainig data for Bio to assume that it would be able to learn anything about the generalization. Some of the categories are also inherently flawed, for instance Other seems to be random, meaning there are very little defining features of the class, just an assortment of object. Glass is moslty broken glass, and YOLO is known to have difficulty with small objects. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COCO evaluation\n",
    "We followed the demo provided above to perform a coco evaluation on the test image annotations provided\n",
    "\n",
    "## Download the test set\n",
    "We used Roboflow to prepare the images for testing like above. Downloading the data set is very similar, but the COCO download must be done as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use this block to download the test set\n",
    "#running this block will download the image and label files in the folder you are in, make sure to do so in the roboflow download dir\n",
    "#import project.model_utils as utils\n",
    "#utils.download_data(apikey=\"tbgnNS8bCW5iRz1lVg3O\", workspace=\"deep-learning-q1acw\",\n",
    "#                   project=\"trash-detection-original-test-images\", version=1, model=\"yolov8\")\n",
    "#utils.download_data(apikey=\"tbgnNS8bCW5iRz1lVg3O\", workspace=\"deep-learning-q1acw\",\n",
    "#                   project=\"trash-detection-original-test-images\", version=1, model=\"coco\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test set preprocessing\n",
    "The test set images and labels are downloaded with the names given by roboflow. \n",
    "We wished to evaluate using both the yolo evaluation methods and COCOeval \n",
    "In order to do so the file names must be changed to the image ids in the annotation files. \n",
    "Run the script below to do so, the folders will need to be updated accordingly. Run the block below, changing the PATH_TO_FOLDER_PATH to the place you downloaded the dataset to, after moving the annotation files to it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run project/processTestset.py PATH_TO_TEST_SET_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# running evaluation\n",
    "We first run yolo evaluation, then the COCOeval function\n",
    "the path to the weights obtained in learning and the path to the test set data.yaml folder should be switched below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run project/evaluate.py PATH_TO_WEIGHTS PATH_TO_TEST_SET_ANOTATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO Evaluation\n",
    "\n",
    "The YOLO evaluation gave a mAP of 0.05 on the test set, which unfortunatly is quite low.\n",
    "The confusion matrix is not surprising, giving similar results to the test set. \n",
    "\n",
    "<figure>\n",
    "    <center><img src=\"imgs/conMatOrigOnTest1.png\" /></center>\n",
    "</figure>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image results\n",
    "\n",
    "Here are some of the results:\n",
    "<div style=\"display: flex;\">\n",
    "  <div style=\"flex: 50%; padding: 10px;\">\n",
    "    <img src=\"imgs/valLabelOrigTrain1.jpg\" alt=\"Image 1\" width=\"100%\">\n",
    "    <p>Labels</p>\n",
    "  </div>\n",
    "  <div style=\"flex: 50%; padding: 10px;\">\n",
    "    <img src=\"imgs/valPredOrigTrain1.jpg\" alt=\"Image 2\" width=\"100%\">\n",
    "    <p>Predictions</p>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "\n",
    "As we can see, the model tends to predict Metals and Plastic, as it is very over represented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coco Evaluation\n",
    "In order to further our understanding, we used the COCOeval functions you referenced us to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script recievs the paths to the ground truth annotations for the test set downloaded from ROBOFLOW\n",
    "# and the predictions generated above\n",
    "# %run project/COCOeval.py PATH_TO_TRUTH_ANNOTAIONS PATH_TO_PREDICTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COCO results:\n",
    "| Metric | IoU Range | Area | Max Dets | Value |\n",
    "|--------|-----------|------|----------|-------|\n",
    "| AP     | 0.50:0.95 | all  | 100      | 0.076 |\n",
    "| AP     | 0.50      | all  | 100      | 0.106 |\n",
    "| AP     | 0.75      | all  | 100      | 0.081 |\n",
    "| AP     | 0.50:0.95 | small| 100      | 0.037 |\n",
    "| AP     | 0.50:0.95 | medium| 100     | 0.124 |\n",
    "| AP     | 0.50:0.95 | large | 100     | 0.112 |\n",
    "| AR     | 0.50:0.95 | all  | 1        | 0.181 |\n",
    "| AR     | 0.50:0.95 | all  | 10       | 0.332 |\n",
    "| AR     | 0.50:0.95 | all  | 100      | 0.386 |\n",
    "| AR     | 0.50:0.95 | small| 100      | 0.276 |\n",
    "| AR     | 0.50:0.95 | medium| 100     | 0.496 |\n",
    "| AR     | 0.50:0.95 | large | 100     | 0.572 |\n",
    "\n",
    " \n",
    " Above we see the results of the model using COCO evaluation. \n",
    " Some notable results include a Average recall of 0.57 with IOU of 0.50:0.95 and a large area, which is actually quite good comparitively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmentations\n",
    "After the experiment on the original dataset we decided to augment the data to see if we could obtain better results. We again used Roboflow to do the following:\n",
    "- Resize all images to 640 $\\times$ 640, the recommeded size for YOLOv8\n",
    "- Auto Orient: Roboflow will automatically orient the images based on the information provided in annotations\n",
    "- Greyscale: We applied grayscale to around 25% of the data set to simulate more lowlight conditions\n",
    "- Shear: We applied a shear of 15° horizontal and 15° vertical, to simulate various photo angles.\n",
    "- Flip: we applied a horizontal and vertical flip, again to simulate different angle\n",
    "\n",
    "Once the augmentations were applied, the new dataset was ready for the model.\n",
    "We redownloaded the new dataset, trained a model and performed the evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = utils.download_data(apikey=\"tbgnNS8bCW5iRz1lVg3O\", workspace=\"deep-learning-q1acw\",\n",
    "#                   project=\"trash-detection-original-train-images\", version=2, model=\"yolov8\")\n",
    "#model = utils.initialize_and_train_model(\"dataset\", 'yolov8x')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run project/evaluate.py PATH_TO_WEIGHTS PATH_TO_TEST_SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run project/COCOeval.py PATH_TO_TRUTH_ANNOTAIONS PATH_TO_PREDICTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmented results\n",
    "\n",
    "We got the following training results and  confusion matrix:\n",
    "\n",
    "\n",
    "<figure>\n",
    "    <center><img src=\"imgs/resultsAugTrain1.png\" /></center>\n",
    "</figure>\n",
    "\n",
    "<figure>\n",
    "    <center><img src=\"imgs/confMatAugTrain1.png\" /></center>\n",
    "</figure>\n",
    "\n",
    "\n",
    "We can see that this is a small improvement, however it still isn't great, with pretty much the same problems as before. Again, Metals and Plastics simply dominate the result. The image predictions show this very well\n",
    "\n",
    "# Image results\n",
    "Here are some of the results:\n",
    "<div style=\"display: flex;\">\n",
    "  <div style=\"flex: 50%; padding: 10px;\">\n",
    "    <img src=\"imgs/valLabelsAugTrain1.jpg\" alt=\"Image 1\" width=\"100%\">\n",
    "    <p>Labels</p>\n",
    "  </div>\n",
    "  <div style=\"flex: 50%; padding: 10px;\">\n",
    "    <img src=\"imgs/valPredAugTrain1.jpg\" alt=\"Image 2\" width=\"100%\">\n",
    "    <p>Predictions</p>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation results:\n",
    "\n",
    "The YOLO evaluation on the test set gave an mAP of 0.10, an improvemnet. We assume that now the Metals and Plastics had enough data to be both really learned and overfit, and as such the results were skewed to it.\n",
    " # COCO\n",
    " The coco evaluation was similar:\n",
    "\n",
    " | Metric | IoU Range | Area | Max Dets | Value |\n",
    "|--------|-----------|------|----------|-------|\n",
    "| AP     | 0.50:0.95 | all  | 100      | 0.066 |\n",
    "| AP     | 0.50      | all  | 100      | 0.097 |\n",
    "| AP     | 0.75      | all  | 100      | 0.072 |\n",
    "| AP     | 0.50:0.95 | small| 100      | 0.021 |\n",
    "| AP     | 0.50:0.95 | medium| 100     | 0.091 |\n",
    "| AP     | 0.50:0.95 | large | 100     | 0.140 |\n",
    "| AR     | 0.50:0.95 | all  | 1        | 0.161 |\n",
    "| AR     | 0.50:0.95 | all  | 10       | 0.316 |\n",
    "| AR     | 0.50:0.95 | all  | 100      | 0.378 |\n",
    "| AR     | 0.50:0.95 | small| 100      | 0.315 |\n",
    "| AR     | 0.50:0.95 | medium| 100     | 0.464 |\n",
    "| AR     | 0.50:0.95 | large | 100     | 0.596 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mid Summary\n",
    "\n",
    "The YOLO model did not achieve amazing results on the TACO data set, however it did not fail abysmally either. \n",
    "It seems that with more images and a larger data set the results could be improved. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Changing the categories\n",
    "\n",
    "As we thought that a major issue was that the categories were not split correctly, we decided to resplit the categories. \n",
    "\n",
    "# Data Preprocessing\n",
    "In order update the categories, we had to do some preproccesing to the TACO dataset. The raw TACO data set comes as multiple batches of images with overlapping names. YOLO requires one folder of images for training, and one for validation. First we consolidated the images into one large folder, updating the image names in the annotation files as well. \n",
    "\n",
    "## Annotation updates\n",
    "We ran a script to change the category names into new ones, updating the relevant fields\n",
    "\n",
    "All of this was done by running the preprocessing script we wrote on the TACO dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# process the TACO dataset, unifying the image folders and categories\n",
    "# in order to run this script, simply provide the path to the folder containing both the TACO images\n",
    "# and annotations. a new folder called images will be created containing the images, and the \n",
    "# annotations folder will be updated inplace\n",
    "# replace DATAFOLDERPATH with the path to your data.\n",
    "# run the split you want\n",
    "# %run project.preprocess_second_split.py DATAFOLDERPATH\n",
    "# %run project.preprocess_third_split.py DATAFOLDERPATH\n",
    "# the categories have to be changed manually in the script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmentation\n",
    "augmentation was done using Roboflow again.\n",
    "\n",
    "# Download\n",
    "The datasets must be downloaded, then used to train again. We note that running this notebook caused the download paths to vary alot, and therefor it is required to manually input the paths to the relevant folders and downloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#Use this block to download the first new split\n",
    "#running this block will download the image and annotation files\n",
    "#import project.model_utils as utils\n",
    "#dataset1 = utils.download_data(apikey=\"tbgnNS8bCW5iRz1lVg3O\", workspace=\"deep-learning-q1acw\",\n",
    "#                   project=\"trash-detection-new-categories\", version=2, model=\"yolov8\")\n",
    "#dataset1 = utils.download_data(apikey=\"tbgnNS8bCW5iRz1lVg3O\", workspace=\"deep-learning-q1acw\",\n",
    "#                   project=\"trash-detection-new-categories\", version=2, model=\"coco\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#Use this block to download the second new split\n",
    "#running this block will download the image and annotation files\n",
    "#import project.model_utils as utils\n",
    "#dataset2 = utils.download_data(apikey=\"tbgnNS8bCW5iRz1lVg3O\", workspace=\"deep-learning-q1acw\",\n",
    "#                   project=\"trash-detection-third\", version=1, model=\"yolov8\")\n",
    "#Use this block to download the second new split\n",
    "#running this block will download the image and annotation files\n",
    "#import project.model_utils as utils\n",
    "#dataset2 = utils.download_data(apikey=\"tbgnNS8bCW5iRz1lVg3O\", workspace=\"deep-learning-q1acw\",\n",
    "#                   project=\"trash-detection-third\", version=1, model=\"coco\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Now you can run a model on one of the splits\n",
    "#the function recieves a path to the data.yaml folder in the downloaded data folder\n",
    "#model = utils.initialize_and_train_model(datapath, 'yolov8x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process anotations and evaluation\n",
    "Much like above the annotaion files must processed than evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run project/processTestset.py PATH_TO_TEST_SET_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run project/evaluate.py PATH_TO_WEIGHTS PATH_TO_TEST_SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run project/COCOeval.py PATH_TO_TRUTH_ANNOTAIONS PATH_TO_PREDICTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "### Attempt number 1:\n",
    "After using the categories provided we attempted to train a model using resplit categories. The Bio category was severly under represented and therefor not detected well. This simply added noise to the confusion matrix.\n",
    "We now split the data into: Glass, Metal and Plastic, Other Plastic, Cigarette, Non Recyclable, Paper and Other. As each category was now more balanced, the classes were not under represented. This seemed to us a better split. \n",
    "The best results we got where:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<figure>\n",
    "    <center><img src=\"imgs/confMatSplit2.png\" /></center>\n",
    "    <figcaption>\n",
    "    Confusion Matrix from the first attempt to split categories\n",
    "    </figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The mAP was much better on the validation test, 0.34. \n",
    "\n",
    "The overall COCO evaluation was worse:\n",
    "| Metric | IoU Range | Area | Max Dets | Value |\n",
    "|--------|-----------|------|----------|-------|\n",
    "| AP     | 0.50:0.95 | all  | 100      | 0.010 |\n",
    "| AP     | 0.50      | all  | 100      | 0.015 |\n",
    "| AP     | 0.75      | all  | 100      | 0.009 |\n",
    "| AP     | 0.50:0.95 | small| 100      | 0.002 |\n",
    "| AP     | 0.50:0.95 | medium| 100     | 0.008 |\n",
    "| AP     | 0.50:0.95 | large | 100     | 0.027 |\n",
    "| AR     | 0.50:0.95 | all  | 1        | 0.064 |\n",
    "| AR     | 0.50:0.95 | all  | 10       | 0.145 |\n",
    "| AR     | 0.50:0.95 | all  | 100      | 0.177 |\n",
    "| AR     | 0.50:0.95 | small| 100      | 0.061 |\n",
    "| AR     | 0.50:0.95 | medium| 100     | 0.172 |\n",
    "| AR     | 0.50:0.95 | large | 100     | 0.289 |\n",
    "\n",
    "performing worse in almost every metric. \n",
    "\n",
    "We noticed that was not getting detected well at all, and assumed that the glass annotations were skewing the results as they make up a large percentage of the dataset.\n",
    "We attempted to improve this in numerous ways. We decided to run the model on larger images, assuming that cigarettes was too small to detect. This took a whole day to train and the results were worse. We attempted to run the project using a verison of yolo that is designed for smaller images, v8-p2, this also did not improve the results. We therefor decided to resplit the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "### Attempt number 2:\n",
    "The results we got above did not satisfy us, and we wanted to refine the split even more. We decided that as well as material, shape and size should be taken into account when splitting the categories. \n",
    "We now split the data into: Bags, Bottles,cups and cans, Non_recyclables, Other, Other plastic, Paper, Small waste. As each category now contatined objects that were similar in many aspects, thhis seemed to us a better split. \n",
    "The best results we got where:\n",
    "\n",
    "\n",
    "<figure>\n",
    "    <center><img src=\"imgs/confMatSplt3.png\" /></center>\n",
    "    <figcaption>\n",
    "    Confusion Matrix from the second attempt to split categories\n",
    "    </figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "\n",
    "The mAP again was better than the original split, at 0.18, but worse that the first split. We did however get a much better COCO evaluation. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COCO EVAL \n",
    "for the third split we got the following COCO evaluation results:\n",
    "| Metric | IoU Range | Area | Max Dets | Value |\n",
    "|--------|-----------|------|----------|-------|\n",
    "| AP     | 0.50:0.95 | all  | 100      | 0.129 |\n",
    "| AP     | 0.50      | all  | 100      | 0.169 |\n",
    "| AP     | 0.75      | all  | 100      | 0.144 |\n",
    "| AP     | 0.50:0.95 | small| 100      | 0.067 |\n",
    "| AP     | 0.50:0.95 | medium| 100     | 0.155 |\n",
    "| AP     | 0.50:0.95 | large | 100     | 0.200 |\n",
    "| AR     | 0.50:0.95 | all  | 1        | 0.216 |\n",
    "| AR     | 0.50:0.95 | all  | 10       | 0.319 |\n",
    "| AR     | 0.50:0.95 | all  | 100      | 0.337 |\n",
    "| AR     | 0.50:0.95 | small| 100      | 0.280 |\n",
    "| AR     | 0.50:0.95 | medium| 100     | 0.358 |\n",
    "| AR     | 0.50:0.95 | large | 100     | 0.520 |\n",
    "\n",
    "We can see that we get slighlty more balanced results, with over al higher metrics and sensitivity. It seems that our resplit resulted in a better overall model. The new split resulted in a model that generally more accurate, with higher accuracy and recall. The model is also more accurate at higher IoU thresholds, and is overall more balanced, providing better results accross multiple metrics. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "Overall, we did not succeed in getting ground breaking results with our model on the TACO dataset. However we also assert that our results are nothing to be ashamed of given the limitations of the task. The Yolo model did quite well for such a limited task. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
